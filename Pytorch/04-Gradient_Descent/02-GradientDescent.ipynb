{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent  For Loss Minimization\n",
    "    :기초적인 최적화 방법  \n",
    "    :주어진 함수를 x로 미분하여 기울기를 활용. learning rate(step-size)와 함께 함수값(출력값)을 최소로 만드는 지점을 찾는 방법  \n",
    "    :Loss function을 최소로하는 세타를 찾는다.  \n",
    "    \n",
    "    주의할 점 : Global Minima & Local Minima  \n",
    "    그러나, 수 많은 차원에서 동시에 local minima를 위한 조건이 만족되기는 어려움.  \n",
    "    \n",
    "    learning rate는 이후 Adam optimizer를 통해서 해결가능하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7280, 0.6265, 0.7596],\n",
       "        [0.0148, 0.4087, 0.4797],\n",
       "        [0.3359, 0.2179, 0.8097]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# AutoGrad\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2058, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 1.2450e-01\n",
      "tensor([[0.6984, 0.5182, 0.4612],\n",
      "        [0.7033, 0.3670, 0.6659],\n",
      "        [0.4502, 0.2685, 0.4801]], requires_grad=True)\n",
      "2-th Loss: 7.5312e-02\n",
      "tensor([[0.5654, 0.4475, 0.4253],\n",
      "        [0.6359, 0.3966, 0.6512],\n",
      "        [0.5057, 0.3866, 0.5734]], requires_grad=True)\n",
      "3-th Loss: 4.5559e-02\n",
      "tensor([[0.4620, 0.3925, 0.3975],\n",
      "        [0.5835, 0.4196, 0.6398],\n",
      "        [0.5489, 0.4785, 0.6460]], requires_grad=True)\n",
      "4-th Loss: 2.7560e-02\n",
      "tensor([[0.3815, 0.3497, 0.3758],\n",
      "        [0.5427, 0.4374, 0.6310],\n",
      "        [0.5825, 0.5499, 0.7024]], requires_grad=True)\n",
      "5-th Loss: 1.6672e-02\n",
      "tensor([[0.3190, 0.3164, 0.3590],\n",
      "        [0.5110, 0.4513, 0.6241],\n",
      "        [0.6086, 0.6055, 0.7463]], requires_grad=True)\n",
      "6-th Loss: 1.0086e-02\n",
      "tensor([[0.2703, 0.2906, 0.3459],\n",
      "        [0.4863, 0.4621, 0.6187],\n",
      "        [0.6289, 0.6487, 0.7805]], requires_grad=True)\n",
      "7-th Loss: 6.1012e-03\n",
      "tensor([[0.2325, 0.2704, 0.3357],\n",
      "        [0.4671, 0.4706, 0.6146],\n",
      "        [0.6447, 0.6823, 0.8070]], requires_grad=True)\n",
      "8-th Loss: 3.6909e-03\n",
      "tensor([[0.2030, 0.2548, 0.3277],\n",
      "        [0.4522, 0.4771, 0.6113],\n",
      "        [0.6570, 0.7085, 0.8277]], requires_grad=True)\n",
      "9-th Loss: 2.2328e-03\n",
      "tensor([[0.1801, 0.2426, 0.3216],\n",
      "        [0.4406, 0.4822, 0.6088],\n",
      "        [0.6665, 0.7288, 0.8438]], requires_grad=True)\n",
      "10-th Loss: 1.3507e-03\n",
      "tensor([[0.1623, 0.2331, 0.3168],\n",
      "        [0.4316, 0.4861, 0.6069],\n",
      "        [0.6740, 0.7446, 0.8563]], requires_grad=True)\n",
      "11-th Loss: 8.1708e-04\n",
      "tensor([[0.1485, 0.2258, 0.3131],\n",
      "        [0.4246, 0.4892, 0.6053],\n",
      "        [0.6798, 0.7569, 0.8660]], requires_grad=True)\n",
      "12-th Loss: 4.9428e-04\n",
      "tensor([[0.1377, 0.2200, 0.3102],\n",
      "        [0.4191, 0.4916, 0.6042],\n",
      "        [0.6843, 0.7665, 0.8735]], requires_grad=True)\n",
      "13-th Loss: 2.9901e-04\n",
      "tensor([[0.1293, 0.2156, 0.3079],\n",
      "        [0.4149, 0.4935, 0.6032],\n",
      "        [0.6878, 0.7740, 0.8794]], requires_grad=True)\n",
      "14-th Loss: 1.8088e-04\n",
      "tensor([[0.1228, 0.2121, 0.3061],\n",
      "        [0.4116, 0.4949, 0.6025],\n",
      "        [0.6905, 0.7797, 0.8840]], requires_grad=True)\n",
      "15-th Loss: 1.0942e-04\n",
      "tensor([[0.1177, 0.2094, 0.3048],\n",
      "        [0.4090, 0.4961, 0.6020],\n",
      "        [0.6926, 0.7842, 0.8876]], requires_grad=True)\n",
      "16-th Loss: 6.6194e-05\n",
      "tensor([[0.1138, 0.2073, 0.3037],\n",
      "        [0.4070, 0.4969, 0.6015],\n",
      "        [0.6942, 0.7877, 0.8903]], requires_grad=True)\n",
      "17-th Loss: 4.0043e-05\n",
      "tensor([[0.1107, 0.2057, 0.3029],\n",
      "        [0.4054, 0.4976, 0.6012],\n",
      "        [0.6955, 0.7905, 0.8925]], requires_grad=True)\n",
      "18-th Loss: 2.4224e-05\n",
      "tensor([[0.1083, 0.2044, 0.3022],\n",
      "        [0.4042, 0.4981, 0.6009],\n",
      "        [0.6965, 0.7926, 0.8941]], requires_grad=True)\n",
      "19-th Loss: 1.4654e-05\n",
      "tensor([[0.1065, 0.2035, 0.3017],\n",
      "        [0.4033, 0.4986, 0.6007],\n",
      "        [0.6973, 0.7942, 0.8954]], requires_grad=True)\n",
      "20-th Loss: 8.8647e-06\n",
      "tensor([[0.1050, 0.2027, 0.3014],\n",
      "        [0.4026, 0.4989, 0.6006],\n",
      "        [0.6979, 0.7955, 0.8965]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
